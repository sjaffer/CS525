\documentclass[11pt,twocolumn]{article}
\usepackage[margin=1in]{geometry}

\title{User Obfuscation on the Web}
\author{Sarah Jaffer\\sjaffer@wpi.edu \and Evan Frenn\\ejfrenn@wpi.edu}
\date{}

\begin{document}

\maketitle

\section{Introduction}
Users everywhere are starting to realize just how much using the internet compromises their privacy.  It is rare to navigate to a website that does not incorporate several tracking cookies and nearly a dozen advertisements.  This is the economic model the internet has adopted and the only say users have had in the decision is in their personal browsing habits.  Targeted advertising is almost ubiquitous at this point but even if a user finds one of these ads appealing, he or she has already paid in personal information just to be marketed to.

Many of these tracking mechanisms are easy to defeat... with a little effort.  Many browser plugins offer protection from advertising or tracking.  However, a user that attempts to go this route may end up with more tools and uncommon browser settings than he or she feels comfortable with and there may still be privacy leaks.

We present a tool, \textit{(we need a name for it)}, which attempts to prevent privacy leakage by using known tracking methods and feeding them false data.  In this way, a user could browse the web comfortably with cookies enabled and not need to worry about their browsing habits and personal information leaking.  This plugin intercepts all of that data and filters out anything compromising, replacing it with a fake identity.

\section{Related Works}

A group of researchers out of Stanford proposed changes to the way that browsers work in order to protect the browser state from attacks on a user's privacy \cite{Shankar:2006:DBB:1180405.1180426}.  The key idea here is their more precise reformulation of the commonly-accepted Same-Origin Principle to the following: Only the site that stores some information in the browser may later read or modify that information.  Starting from that basis, it would be possible to make the Internet a much safer place by enforcing these policies in the browser and by doing that, force websites to adhere to them.

In a similar vein, Doppleganger is a browser plugin designed to explore the use of specific cookies at runtime and discard `useless' cookies \cite{Jackson:2006:PBS:1135777.1135884}.  They hold that any cookie that does not produce a noticeable change in the user's browser experience by existing is there for the express purpose of violating user privacy.

Another browser plugin which goes in a different direction is \textit{PrivaCookie}, which allows some (hopefully controlled) privacy leakage in exchange for targeted advertising, but limits the damage by only allowing subsets of websites to obtain the information.  Their approach targets cookie-sending and URL arguments \cite{freudiger2009towards}.

Chen and Rea look at users' reactions to privacy violations, including potential methods they use to mitigate it, such as falsifying identification, modifying their identification between different websites to mitigate linkage, and passively blocking certain undesired application capabilities (blocking third-party cookies, for instance).  Their study claims that falsifying information provides the greatest anonymity on the internet \cite{chen2004protecting}.
	
A group out of CMU put together a usability study on a variety of commonly-used privacy protection tools: opt-out cookies, browser settings, and blocking tools.  They concluded that these tools generally come with inappropriate settings, that users do not have enough domain knowledge to use them properly, confounded by the fact that the the tools do not give users the resources or feedback in order to teach them, and that a lot of these tools broke websites beyond user tolerance.  This results in users not being able to protect themselves as well as they intend or would like.
	
Krishnamurthy, Naryshkin, and Wills systematically analyzed privacy leakage from first party to third party sites for the top 10 websites of 10 Alexa categories \cite{Krishnamurthy2011Privacy}. They focused on account registration and usage for these 100 sites and added another 20 sites of online social networks (OSN) and health sites and relaxed the requirement of registration. Results showed  privacy leakage of account registration through HTTP GET requests, the Referer header, and first party cookies to hidden third parties, to name a few. The researchers noted possible linkage of user activity without cookies by using IP address, with references to previous studies that have shown high probability of a user retaining their IP address over short periods of time.Commercially available products aimed at protecting user privacy were analyzed and the researchers found that none provided full coverage of all the possible points of leakage. These results are useful in pointing out current products available and their methods of protecting user privacy, as well as providing further attack vectors to consider when trying to protect user privacy.
	
A continuation of this research can be found in \cite{Wills2011Identifying}. The author looked at the avenues available to users and found the available actions insufficient in protecting user privacy. Possible actions that could be used by first party sites to protect user privacy were reviewed. These techniques included the migration of HTTP GET transactions to HTTP POST, refrain from using hidden third parties, and avoid putting private information in page titles.

A study performed by Peter Eckersley out of the Eltronic Frontier Foundation looked at the possibility of using web browser characteristics as a unique identifier of users \cite{Eckersley:2010:UYW:1881151.1881152}. Eckerslery collected data from users participating in his study by visiting the website panopticlick.eff.org and found that one in 286,777 browsers share the same unique identifiers. If the browser had Flash or Java enabled, 94.2\% of these browsers were uniquely identifiable. The author was then able to create a heuristic that was able to correlate changes to a browsers characteristics by a user 65\% of the time, with a 99.1\% accuracy rate. Characteristics included in the study were User Agent, HTTP ACCEPT headers, whether cookies were enabled, browser plugins and system fonts. Data collection techniques not implemented comprised of super cookies including Microsoft's ActiveX and Silverlight APIs, detection of system fonts when Javascript and Flash were disabled, TCP fingerprinting, or further customized detection of plugins for IE, although the author made note that these data points could yield greater fingerprinting resolution. While the author admitted browser fingerprinting is not currently a technique publicized as being used by aggregators, its use is possible in the near future and should be considered as an avenue of privacy leakage.
	
A group of researches out of UC Berkley analyzed Flash cookie use by the ``top 100 sites on the Internet'', to look at the relation of Flash cookie use to user privacy \cite{Soltani2009Flash}. They analyzed the use of a unique identifier in Flash cookies by both first and third party sites to create a redundant tracking method to HTTP cookies. Results showed that 31 of the top 100 sites had matching HTTP and Flash cookie values and that most of these cookies were placed by 3rd party providers. The authors then checked for respawning of HTTP cookies through use of Flash cookies by storing HTTP cookie values, erasing all HTTP cookies from their test machine, and revisiting the target sites. The study found that Flash cookie unique IDs were being used to restore deleted HTTP cookies. In one case, the study found that QuantCast was using Flash cookies on a machine with an NAI opt-out cookie set of Quantcast. Once the HTTP cookies on the machine were cleared (including the opt-out cookie), an HTTP matching the Flash cookie value was then placed on the test machine. In summary, Flash cookies are a persistent threat to user anonymity and should be an attack vector we aim to mitigate for our project.

\section{Methodology}
We implemented our design using an extension for the Chrome browser as the client-side model. The client application was written in Javascript and interacts with the browser using Chrome's APIs, including experimental APIs recently added by Google. The extension performs two important functions. First, it protects user privacy by filtering both personally identifiable information and user identification data intended for third party sites and injecting spoofed values in their place. Second, it builds valid search histories based on websites visited by actual users and pairs this with a forged identity to be used during the injection period. Both steps are discussed below in their respective sections.

\subsection{PII and User Data Filtering}
When connecting to a first party site (the site the user explicitly choose to visit by either clicking on a link or typing the URL into the address bar in the browser) generally many third party sites are implicitly involved in loading the content of the page displayed to the user. Many of these third party sites are ad providers, who store cookies on the users machine to provide unique identification for later communications. In order to uniquely identify a user and perform ad targeting, third party sites generally user third party cookies, as well as browser history and first party cookies (generally accessed through hidden third party URLs). Our application intercepts requests from the browser being made to these third party sites and swaps out the real users cookies and browser history with spoofed versions. Whenever a first party site is visited, the real users profile will be used, removing the possibility of degradation of user experience usually seen when first party cookies are disabled and also allowing the user to comply with the first party sites' terms of use.

The profile swapping of actual user data with spoofed user data and its reciprocal are done using the chrome APIs. Because any third party code is being executed on spoofed user data, no real user data can be leaked. During the interception of URL requests, the extension will use pattern matching to find any occurrences of PII in the HTTP requests and replace them with the spoofed data. The extension is optimized to allow all first party requests to finish prior to swapping profiles and unblocking third party requests. It also clears the browser cache after loading a new profile, to guarantee the corresponding sites are receiving the correct profile data. An example of the functions performed by the extension and the corresponding browser events may help the clarify the overall design of the extension. When the user navigates to a site, the extension intercept the first request (and all subsequent requests), using Chrome's webRequest API. It then makes note of the first-party characteristics, swaps in the actual user profile (including cookies and browsing history), and clears the browser cache. It subsequently intercepts and blocks all third party requests until the first party objects have been loaded. Once all first party communication has finished, the extension swaps out the actual user profile with the spoofed user profile, and unblocks third party requests. Any third party request that is intercepted is also be scanned for the user's PII, which will be replaced.

\subsection{Profile Creation}
While the extension could use spoofed profiles that are entirely fabricated, we believe this represents a vulnerability in the protection of user privacy. This belief stems from the difficulty of creating profiles that have the correct randomness and distribution of actual user data, and the requirement that the method used must either be self learning or frequently updated to keep up with an ever changing environment. Because of these obstacles, we believe it best to use spoofed profiles that are based on real user data. This real user data is used to create the browser history of the spoofed profiles that are injected to the third party sites. The extension is tasked with periodically extracting the browser history of its users into a template. This template does not contain the specific sites visited by the user, but rather subcategories of these sites. This is to remove any reservations users may have about their browser histories being transmitted to other users.

Once a template is created, the extension transmits it to our server. The server is responsible for storing templates and matching them up with fictitious identities (first and last name, phone number, zipcode). The identities are created in such a manner as to avoid representing a real identity. Once a profile has been generated, constituting a template and a fabricated identity, the server stores the profile until it receives a request by an extension. When an extension receives a new profile from the server, it resolves the browser history template to specific sites within the subcategories and builds a spoofed profile used when injecting spoofed browser history and cookies, as mentioned above. 


\section{Results}
This tool was evaluated based on three metrics: the performance of the tool, the accuracy of the tool in determining what is a first party versus third party site, and the accuracy in detecting and replacing personally identifiable information.

Performance was measured as the time increase in fully loading a web page with the tool versus without it.  This was an important metric, as the tool should seamlessly integrate into a user's browsing experience without imposing an unreasonable cost in page fetch times.  On average, over a series of \textit{xx} fetches automated over several days, there was an increase in page load time of \textit{xx}.

For both accuracy tests, the tool was deployed on \textit{xx} machines dispersed across the PlanetLab network.  The packet captures were logged and stored and analysed using a combination of pattern matching and manual inspection, searching for pieces of information that were supposed to remain hidden from third-party sites.

The first accuracy measurement was based on the number of cookies leaked to third-party sites (false negatives) and the number of first-party sites treated like third-party sites (false positives).  These results are summarized in table \textit{xx}.

The referrer links were scanned for `real' data that was missed by the tool's replacement mechanism.  \textit{xx} of the results accidentally leaked this information.  There is potentially more data leaked this way that is not obvious, as it is encoded in an unexpected way.

All of these values fall within reasonable margins of error for such a tool.


\section{Conclusion}
In this work, we have shown that it possible to prevent PII from being leaked to third parties without a user's consent. To the best of our knowledge, it is this first tool to protect a user's private data from third party ad providers and aggregators through the use of a spoofed user profile. As the results show, it is possible to successfully swap out spoofed user data without incurring large performance overhead. The technique used require minimal user intervention and therefore, we believe our approach provides a realistic option that is employable by the general public. While the performance of our tool relies on its ability to correctly identify hidden third parties, advances in this process can easily be integrated with our application to provide greater precision in protecting a user's private data. Previous research has shown that there are attack vectors which are currently not mitigated by our tool, including the use of flash cookies. In the future we plan to provide protection from PII leakage through flash data and believe the platform detailed in this article provides a strong basis for future development in this area.


\bibliographystyle{abbrv}
\bibliography{litrev}

\end{document}